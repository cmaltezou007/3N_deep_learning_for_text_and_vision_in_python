{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHj7i4jGYH/+ml9+ly5ybf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"N6YRSPPEQysG"},"outputs":[],"source":["# All necessary libraries for this project\n","\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from sklearn.metrics import classification_report\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image"]},{"cell_type":"code","source":["# Mounting/linking my Google Drive account with this Google Colab project.\n","\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT, force_remount=True)\n","\n","path = \"/content/drive/My Drive/Essex Summer School 2025/3N Deep Learning for Text and Vision/Exam/\"\n","os.chdir(path)\n","print(\"Current working directory:\", os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJZhMGCfQ-sr","executionInfo":{"status":"ok","timestamp":1755543135082,"user_tz":-60,"elapsed":16608,"user":{"displayName":"Constantina Maltezou","userId":"04281137762355711390"}},"outputId":"0cfb334f-b85c-4b86-c081-657ccd6d6cef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/My Drive/Essex Summer School 2025/3N Deep Learning for Text and Vision/Exam\n"]}]},{"cell_type":"markdown","source":["Retrieving and filtering the Fashion-MNIST data."],"metadata":{"id":"v0I9BFh2REgV"}},{"cell_type":"code","source":["# Retrieving and filtering the Fashion-MNIST data.\n","\n","# Cnverting each image (which is a PIL image or numpy array) into a PyTorch tensor.\n","transform_images = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Downloading the Fashion-MNIST image dataset, and transforming all images intented for training and testing.\n","full_training_set = datasets.FashionMNIST(root = \"./Vision_data\", train = True, download = True, transform = transform_images)\n","full_test_set = datasets.FashionMNIST(root = \"./Vision_data\", train = False, download = True, transform = transform_images)\n","\n","fashion_categories = full_training_set.classes\n","print(\"\\nFashion categories to choose from:\\n\", fashion_categories)\n","\n","# Choosing three classes I like and dislike for the sentiment analysis.\n","positive_categories = [3, 4, 5]  # dress, coat, sandal\n","negative_categories = [1, 6, 9]  # trouser, shirt, ankle boot\n","\n","class_map = {1: 0, 6: 0, 9: 0, 3: 1, 4: 1, 5: 1}\n","\n","\n","def filter_and_relabel(dataset: Dataset) -> Dataset:\n","  \"\"\"\n","    Creating a new dataset by selecting only the images from the chosen fashion categories,\n","    and remapping their labels into a binary format.\n","  \"\"\"\n","  data = []\n","  targets = []\n","  for img, label in dataset:\n","      if label in class_map:\n","          data.append(img)\n","          targets.append(class_map[label])\n","  # Stack into tensors\n","  data_tensor = torch.stack(data)\n","  targets_tensor = torch.tensor(targets)\n","  return torch.utils.data.TensorDataset(data_tensor, targets_tensor)\n","\n","\n","training_subset = filter_and_relabel(full_training_set)\n","test_subset = filter_and_relabel(full_test_set)\n","\n","# Loading batches of X images at a time for training (random order), and testing (not shuffled).\n","training_loader = DataLoader(training_subset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_subset, batch_size=1000, shuffle=False)\n","\n","\n","# Verifying whether the final dataset is balanced or skewed.\n","print(\"\\nTrain label counts:\", torch.bincount(training_subset.tensors[1]))\n","print(\"\\nTest label counts:\", torch.bincount(test_subset.tensors[1]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiZfhF6tRGKF","executionInfo":{"status":"ok","timestamp":1755545084665,"user_tz":-60,"elapsed":18252,"user":{"displayName":"Constantina Maltezou","userId":"04281137762355711390"}},"outputId":"0a1d5f28-2803-450c-fc82-68234b72f362"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Fashion categories to choose from:\n"," ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","Train label counts: tensor([18000, 18000])\n","\n","Test label counts: tensor([3000, 3000])\n"]}]},{"cell_type":"markdown","source":["Building and training my custom CNN model for sentiment classification on the Fashion-MNIST data.\n","\n","* My model uses two convolutional layers to firstly capture edges and then textures - I could potentially increase the conv layers to capture more details / complex visual features from the clothing images, but my machine can't handle this much processing.\n","* I increased the amount of filters in the second conv layer to 32 from 16, so the CNN model can learn more complex details.\n","* Since max pooling layers reduce spatial dimensions it can help with efficiency in learning the most important aspects (no need to focus on other details).\n","* Dropout helps prevent overfitting.\n","* Then the flattened output is passed through two fully connected layers to predict binary sentiment labels (positive vs negative) by mapping the extracted features to binary sentiment labels.\n","* I used ReLU activations to introduce non-linearity in learning, and the Adam optimiser with cross-entropy loss."],"metadata":{"id":"mS3zeU5zULHf"}},{"cell_type":"code","source":["# Building and training my custom CNN model.\n","\n","class custom_CNN_model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n","        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout = nn.Dropout(0.25) # To reduce overfitting.\n","\n","        # Using a dummy input to calculate the flattened size after conv layers\n","        with torch.no_grad():\n","            dummy_input = torch.zeros(1, 1, 28, 28)\n","            dummy_out = self._forward_features(dummy_input)\n","            flattened_size = dummy_out.view(-1).shape[0]\n","\n","        self.fc1 = nn.Linear(flattened_size, 128)\n","        self.fc2 = nn.Linear(128, 2) # 2 binary categories for my model.\n","\n","    def _forward_features(self, x):\n","        x = F.relu(self.conv1(x))   #\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self._forward_features(x)\n","        self.feature_map = x            # I need them for Grad-CAM\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        return self.fc2(x)\n","\n","\n","model = custom_CNN_model()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Training the model through batches of images, over just 3 full cycles (epochs), otherwise\n","# my machine can't handle it.\n","for epoch in range(3):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(training_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_fn(output, target) # Calculating the loss / how wrong the predicitons are.\n","        loss.backward() # Calculating the gradients.\n","        optimizer.step() # and updating my model's weights.\n","    print(f\"Epoch {epoch + 1} done\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YNFnsr_nUKPL","executionInfo":{"status":"ok","timestamp":1755545248822,"user_tz":-60,"elapsed":95607,"user":{"displayName":"Constantina Maltezou","userId":"04281137762355711390"}},"outputId":"d406c316-d0dc-4bfe-e091-d518c22f83c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 done\n","Epoch 2 done\n","Epoch 3 done\n"]}]},{"cell_type":"markdown","source":["Evaluating the model."],"metadata":{"id":"FB8qDGj_qIjD"}},{"cell_type":"code","source":["\n","model.eval()\n","all_preds, all_labels = [], []\n","\n","with torch.no_grad():\n","    for data, labels in test_loader:\n","        data, labels = data.to(device), labels.to(device)\n","        outputs = model(data)\n","        preds = outputs.argmax(dim=1)\n","        all_preds.extend(preds.cpu())\n","        all_labels.extend(labels.cpu())\n","\n","# Printing final evaluation results from the classification report.\n","report = classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"], output_dict = True)\n","print(\"Accuracy:\", report[\"accuracy\"])\n","print(\"F1-score (macro):\", report[\"macro avg\"][\"f1-score\"])\n","print(\"The current results show that the sentiment analysis model achieved an accuracy of 96% and a macro-averaged F1-score of 96% on the Fashion-MNIST test set.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrblDFepqFOL","executionInfo":{"status":"ok","timestamp":1755545564725,"user_tz":-60,"elapsed":2780,"user":{"displayName":"Constantina Maltezou","userId":"04281137762355711390"}},"outputId":"7db1298e-d285-4b42-fe03-85a1543ef52e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9578333333333333\n","F1-score (macro): 0.9578318984465444\n","The current results show that the sentiment analysis model achieved an accuracy of 96% and a macro-averaged F1-score of 96% on the Fashion-MNIST test set.\n"]}]},{"cell_type":"markdown","source":["I'm using Grad-CAM visualisation to highlight where the CNN is paying attention to when it makes a decision.\n","\n","* To visualize what the model focuses on, I applied Grad-CAM to one of the test images, to highlight important regions in the image that influenced the model's prediction.\n","* The resulting heatmap confirms that the model attends to meaningful features such as the shape and structure of the clothing (i.e., the silhouette of the ankle boot in this instance). Seems to heavily focus on potentially areas that appear quite different, higher distinguishable factor between other shoes, hence seems to be focusing more on the height and back of the ankle boot?"],"metadata":{"id":"uyxZvQBCVhp5"}},{"cell_type":"code","source":["# Using Grad-CAM visualisation to highlight where the CNN is paying attention to when it makes a decision.\n","\n","# Hook to capture gradients and activations\n","gradients = []\n","\n","def save_gradient(grad):\n","    gradients.append(grad)\n","\n","\n","sample_img, label = next(iter(test_loader))\n","sample_img = sample_img[0].unsqueeze(0).to(device)  # pick first image (ankle boot, negative), 7 is the sandal (positive).\n","label = label[0].item() # 0 for negative, 1 for positive.\n","\n","# Forward pass stage\n","# Here's where I'm actually passing the chosen image through my CNN network to classify it either as positive or negative,\n","# and checking which sentiment class it has assigned to it.\n","model.zero_grad()\n","output = model(sample_img)\n","pred_class = output.argmax(dim = 1).item()\n","\n","model.feature_map.register_hook(save_gradient)\n","\n","# Backward pass for predicted class, to check which details/pixels actually mattered in its classification.\n","class_score = output[0, pred_class]\n","class_score.backward()\n","\n","\n","# Shape: [C, H, W] - Channels, Height, Width.\n","grads_val = gradients[0][0].cpu().numpy()\n","feature_map = model.feature_map[0].detach().cpu().numpy()\n","\n","# Identifying the importance of each feature map channel, and\n","# combining them into a 2D heatmap through Grad-CAM vectorisation, to show the areas\n","# my model focused its attention on\n","weights = np.mean(grads_val, axis=(1, 2))\n","cam = np.sum(weights[:, None, None] * feature_map, axis=0)\n","\n","# Finally, getting the image back to its original size\n","cam = np.maximum(cam, 0)\n","cam = cam / cam.max()\n","cam = Image.fromarray(np.uint8(255 * cam)).resize((28, 28), Image.Resampling.BICUBIC)\n","cam = np.array(cam)\n","\n","# ...and plotting the heatmap on top of original image\n","plt.imshow(sample_img.cpu().squeeze(), cmap='gray')\n","plt.imshow(cam, cmap='jet', alpha=0.4)  # Overlay heatmap\n","plt.title(f\"Grad-CAM for predicted class {pred_class}\")\n","plt.axis('off')\n","plt.show()\n","\n","\n","print (\"The model has correctly predicted a negative sentiment of zero for the 'ankle boot' clothing image, matching it's earlier accuracy and F1 scores.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"0sfKPqJYV6D0","executionInfo":{"status":"ok","timestamp":1755548583923,"user_tz":-60,"elapsed":254,"user":{"displayName":"Constantina Maltezou","userId":"04281137762355711390"}},"outputId":"65d2efd7-7f97-49d9-a7df-ee4cf9feb48d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The model has correctly predicted a negative sentiment of zero for the 'ankle boot' clothing image, matching it's earlier accuracy and F1 scores.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HzyG3GqPV-cc"},"execution_count":null,"outputs":[]}]}